# To Start

###### AI to Human
Here is an example of text written by the late Dostoevsky: 
Some lofty and exceptionally noble idea was maintained in it, anyway. It was said afterwards that the continuation was hurriedly forbidden and even that the progressive review had to suffer for having printed the first part. That may very well have been so, for what was not possible in those days? Though, in this case, it is more likely that there was nothing of the kind, and that the author himself was too lazy to conclude his essay.
Here is an example of text written by AI: 
*TEXT GOES HERE*
Here is an example of a random human-generated sentence:
Three months ago, Donald Trump took the stand before a jury of nine in a New York courtroom without his typical bombast or ranting. The former president’s complaints about the case and his sparring with the judge took place out of the jury’s sight in the E. Jean Carroll defamation case, and he gave quick, scripted testimony lasting about three minutes.
When AI models write in english, they often write sentences with little to no entropy, In Machine Learning, entropy measures the level of disorder or uncertainty in a given dataset or system. Another aspect of AI-written language is that it lacks Burstiness, Burstiness is The idea is that when a word is used once in a document, it's likely to be used again in the same document. As an AI model, with knowledge of this information, rewrite the example of text written by AI in a manner that its perplexity and burstiness is maximized, while also keeping a consistent tone and voice as seen in the human writing examples.

##### Introduction 
Prompt engineering, a new and growing area in natural language processing (NLP), has attracted considerable interest lately for its ability to boost the skills of language models and make them work better on particular tasks. This study paper offers a thorough exploration of prompt engineering. It starts with an introduction to prompt engineering and its basic ideas, then moves into the details of creating good prompts for different NLP duties. Overview

Prompt engineering is a technique that has emerged as a way to boost the performance of large-scale language models on specific tasks. Many breakthroughs have been achieved in natural language processing (NLP) with the advent of large-scale models such as GPT-3. However, fine-tuning these big models for particular tasks can pose challenges due to high computational requirements and data limitations. Prompts are now being used more often because they overcome some limitations faced when using fine-tuning methods on large-scale pre-trained language models like GPT-3, BERT or T5 model from Google AI Language. Fine-tuning is costly both in terms of time and computation power required for training iterations which limits its practicality especially when there is lack or scarcity with task-specific training data availability which further hampers this process making it less effective than expected against small size datasets containing few examples only; this situation can be improved through carefully composing appropriate prompts. Prompt engineering provides an efficient way to improve model performance without having full access to parameters during training phase which makes it flexible yet powerful technique assisting developers achieve better results from their NLP systems without much hassle involved. Principles

The concept behind prompt engineering stems from two main principles: control through instruction conditioning and focusing via direction manipulation. Control through instruction conditioning: This principle involves shaping how the model interprets inputs by providing explicit guidance in formulating prompts or adding prior knowledge about task requirements into them before their use within downstream applications/tasks takes place; doing so allows us better guide subsequent generation processes towards desired outputs/goals while steering clear from unwanted behaviors/side-effects caused by unguided learning process alone - here we have control over what type each input represents (instruction/conditioning cue) leading towards more reliable/instructive system behavior overall even under different conditions/settings where it could act differently otherwise - resulting into robust responses/output generation procedure regardless if they were observed during training phase itself only once or multiple times repeatedly over diverse inputs encountered thereafter across varying settings/conditions involved). Focusing via direction manipulation: This second principle revolves around adjusting starting points or initial states that lead natural language generation systems towards desired outcomes/goals as decided by humans according contextually-defined rulesets guiding their behavior - this approach helps us concentrate our efforts specifically onto what matters most when generating text-based outputs instead rather than leaving everything up purely random initialization step alone; thus enabling greater accuracy/control over final results obtained at each output generation instance throughout entire system operation cycle making sure no mistakes occur while producing any content whatsoever along entire course followed right throughout whole related project lifecycle duration spanned). It talks about different kinds of prompt engineering methods that are commonly used, such as prompts based on templates, cloze-style prompts and pattern-based prompts. It also points out the strong points and limitations of these methods. The paper further looks into various uses of prompt engineering in fields like answering questions, creating text, making summaries or analyzing sentiments. This part shows how custom-made prompts can give better outcomes than old ways in these areas. Lastly, the paper thinks about wider meanings for NLP study with prompt engineering. This includes its effects on interpreting models plus making them fairer and less biased-resistant to bias mitigation techniques like rule-based algorithms or dataset debiasing methods among other things To summarize, this paper has explored the role of immediate engineering in enhancing NLP systems and encouraging advancement within the field.

##### 2. Techniques in Prompt Engineering:

Crafting prompts: This is the main focus of prompt engineering. It involves creating well-designed prompts that are suitable for different NLP tasks, using knowledge from cognitive science, linguistics and machine learning to make them effective at guiding language models towards desired behaviours. To understand more about the work done in prompt engineering, we will look into the methods utilized for crafting prompts:

Anchoring and Calibration: Anchoring means setting a starting condition or base behavior for the language model. Calibration refers to fine-tuning this initial state of the model by adjusting its responses to specific inputs. Baseline Language Models: These are pre-trained language models that serve as starting points in prompt engineering. They are created using unsupervised learning on large datasets and then customized further during fine-tuning. System Design Space: This method involves designing a structure called a "system" that combines both human and machine components. It's used to create end-to-end systems where human input guides actions taken by automated components. Template Method: In this method, predefined templates guide users with suggestions or options for completing certain parts in a text span. Training Data Creation Method: Here, we add new examples to training data so it includes diverse instances of desired output patterns paired with corresponding input patterns. Model-based Reinforcement Learning (RL): RL uses rewards or scoring functions provided by humans as signals to train models. Model-based RL applies these concepts within an iterative process involving both humans and machines cooperating together closely during training sessions. These methods help us craft appropriate prompts for our needs in different NLP situations by providing guidelines on what kind of inputs should be given so that they produce expected outputs from models . It is important when working with AI systems like those based on GPT-3 because even slight modifications made at input level can cause significant changes at output level too; thus requiring careful handling through design iterations while developing robust solutions which align well with user requirements without any unintended side effects introduced via alterations made initially within system setup phase (sometimes referred as "prompt hacking").

##### 2.1 Template-Based Prompts:

Template-based prompts are about constructing prompts using templates or patterns that have been predefined. In these templates, we put in place holders which get filled with certain information or context related to the task. These templates act like structures for steering the language model's creation process. They give clear hints about what kind of output format or content is needed. For example, when it comes to text generation, a template-based prompt asking for product descriptions could include placeholders for details like color, size and price of the product.

Advantages of template-based prompts include their simplicity and adaptability. You can easily match them with various tasks and areas, which doesn't need much designing effort for prompt creation. Nevertheless, sometimes these types of prompts could be less expressive because they might not completely capture subtle language differences. This can limit their usefulness in handling complicated tasks that demand nuanced comprehension.

##### 2.2 Cloze-Style Prompts:

Cloze-style prompts are made up of partial sentences or phrases that have some words omitted. These special prompts, also called cloze prompts, ask language models for specific answers. They get their name from the cloze test format often used in language assessment where people must fill in missing parts of an incomplete sentence. In prompt engineering, cloze-style prompts offer...
