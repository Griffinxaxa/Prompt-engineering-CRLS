# To Start
# Wow... just wow.. this is the worst research I've ever read...:

###### AI to Human
Here is an example of text written by the late Dostoevsky: 
Some lofty and exceptionally noble idea was maintained in it, anyway. It was said afterwards that the continuation was hurriedly forbidden and even that the progressive review had to suffer for having printed the first part. That may very well have been so, for what was not possible in those days? Though, in this case, it is more likely that there was nothing of the kind, and that the author himself was too lazy to conclude his essay.
Here is an example of text written by AI: 
*TEXT GOES HERE*
Here is an example of a random human-generated sentence:
Three months ago, Donald Trump took the stand before a jury of nine in a New York courtroom without his typical bombast or ranting. The former president’s complaints about the case and his sparring with the judge took place out of the jury’s sight in the E. Jean Carroll defamation case, and he gave quick, scripted testimony lasting about three minutes.
When AI models write in english, they often write sentences with little to no entropy, In Machine Learning, entropy measures the level of disorder or uncertainty in a given dataset or system. Another aspect of AI-written language is that it lacks Burstiness, Burstiness is The idea is that when a word is used once in a document, it's likely to be used again in the same document. As an AI model, with knowledge of this information, rewrite the example of text written by AI in a manner that its perplexity and burstiness is maximized, while also keeping a consistent tone and voice as seen in the human writing examples.

##### Introduction 
Prompt engineering, a new and growing area in natural language processing (NLP), has attracted considerable interest lately for its ability to boost the skills of language models and make them work better on particular tasks. This study paper offers a thorough exploration of prompt engineering. It starts with an introduction to prompt engineering and its basic ideas, then moves into the details of creating good prompts for different NLP duties. Overview

Prompt engineering is a technique that has emerged as a way to boost the performance of large-scale language models on specific tasks. Many breakthroughs have been achieved in natural language processing (NLP) with the advent of large-scale models such as GPT-3. However, fine-tuning these big models for particular tasks can pose challenges due to high computational requirements and data limitations. Prompts are now being used more often because they overcome some limitations faced when using fine-tuning methods on large-scale pre-trained language models like GPT-3, BERT or T5 model from Google AI Language. Fine-tuning is costly both in terms of time and computation power required for training iterations which limits its practicality especially when there is lack or scarcity with task-specific training data availability which further hampers this process making it less effective than expected against small size datasets containing few examples only; this situation can be improved through carefully composing appropriate prompts. Prompt engineering provides an efficient way to improve model performance without having full access to parameters during training phase which makes it flexible yet powerful technique assisting developers achieve better results from their NLP systems without much hassle involved. Principles

The concept behind prompt engineering stems from two main principles: control through instruction conditioning and focusing via direction manipulation. Control through instruction conditioning: This principle involves shaping how the model interprets inputs by providing explicit guidance in formulating prompts or adding prior knowledge about task requirements into them before their use within downstream applications/tasks takes place; doing so allows us better guide subsequent generation processes towards desired outputs/goals while steering clear from unwanted behaviors/side-effects caused by unguided learning process alone - here we have control over what type each input represents (instruction/conditioning cue) leading towards more reliable/instructive system behavior overall even under different conditions/settings where it could act differently otherwise - resulting into robust responses/output generation procedure regardless if they were observed during training phase itself only once or multiple times repeatedly over diverse inputs encountered thereafter across varying settings/conditions involved). Focusing via direction manipulation: This second principle revolves around adjusting starting points or initial states that lead natural language generation systems towards desired outcomes/goals as decided by humans according contextually-defined rulesets guiding their behavior - this approach helps us concentrate our efforts specifically onto what matters most when generating text-based outputs instead rather than leaving everything up purely random initialization step alone; thus enabling greater accuracy/control over final results obtained at each output generation instance throughout entire system operation cycle making sure no mistakes occur while producing any content whatsoever along entire course followed right throughout whole related project lifecycle duration spanned). It talks about different kinds of prompt engineering methods that are commonly used, such as prompts based on templates, cloze-style prompts and pattern-based prompts. It also points out the strong points and limitations of these methods. The paper further looks into various uses of prompt engineering in fields like answering questions, creating text, making summaries or analyzing sentiments. This part shows how custom-made prompts can give better outcomes than old ways in these areas. Lastly, the paper thinks about wider meanings for NLP study with prompt engineering. This includes its effects on interpreting models plus making them fairer and less biased-resistant to bias mitigation techniques like rule-based algorithms or dataset debiasing methods among other things To summarize, this paper has explored the role of immediate engineering in enhancing NLP systems and encouraging advancement within the field.

##### 2. Techniques in Prompt Engineering:

Crafting prompts: This is the main focus of prompt engineering. It involves creating well-designed prompts that are suitable for different NLP tasks, using knowledge from cognitive science, linguistics and machine learning to make them effective at guiding language models towards desired behaviours. To understand more about the work done in prompt engineering, we will look into the methods utilized for crafting prompts:

Anchoring and Calibration: Anchoring means setting a starting condition or base behavior for the language model. Calibration refers to fine-tuning this initial state of the model by adjusting its responses to specific inputs. Baseline Language Models: These are pre-trained language models that serve as starting points in prompt engineering. They are created using unsupervised learning on large datasets and then customized further during fine-tuning. System Design Space: This method involves designing a structure called a "system" that combines both human and machine components. It's used to create end-to-end systems where human input guides actions taken by automated components. Template Method: In this method, predefined templates guide users with suggestions or options for completing certain parts in a text span. Training Data Creation Method: Here, we add new examples to training data so it includes diverse instances of desired output patterns paired with corresponding input patterns. Model-based Reinforcement Learning (RL): RL uses rewards or scoring functions provided by humans as signals to train models. Model-based RL applies these concepts within an iterative process involving both humans and machines cooperating together closely during training sessions. These methods help us craft appropriate prompts for our needs in different NLP situations by providing guidelines on what kind of inputs should be given so that they produce expected outputs from models . It is important when working with AI systems like those based on GPT-3 because even slight modifications made at input level can cause significant changes at output level too; thus requiring careful handling through design iterations while developing robust solutions which align well with user requirements without any unintended side effects introduced via alterations made initially within system setup phase (sometimes referred as "prompt hacking").

##### 2.1 Template-Based Prompts:

Template-based prompts are about constructing prompts using templates or patterns that have been predefined. In these templates, we put in place holders which get filled with certain information or context related to the task. These templates act like structures for steering the language model's creation process. They give clear hints about what kind of output format or content is needed. For example, when it comes to text generation, a template-based prompt asking for product descriptions could include placeholders for details like color, size and price of the product.

Advantages of template-based prompts include their simplicity and adaptability. You can easily match them with various tasks and areas, which doesn't need much designing effort for prompt creation. Nevertheless, sometimes these types of prompts could be less expressive because they might not completely capture subtle language differences. This can limit their usefulness in handling complicated tasks that demand nuanced comprehension.

##### 2.2 Cloze-Style Prompts:

The style of cloze prompts refers to the use of partial sentences or phrases with missing words, also called cloze prompts, to get particular responses from language models. These kind of prompts are inspired by the format found in a type of language test called a "cloze test." In this kind of test, people must complete sentences that are not finished by filling in the gaps with appropriate words. When we discuss prompt engineering for different tasks like creating text or classification jobs , using these types of cloze-style prompts offers a particular structure for guiding how language models produce or classify things.

The power of cloze-style prompts is that they guide language models to pay attention to important surrounding details and use this information in their educated guesses about the missing parts. When you give a prompt with incomplete sentence or phrase, it makes the model think about context clues and sentence structure for creating possible completions. This helps in producing more precise outputs that match well with the given situation.

Prompts in the style of cloze can be very helpful when the needed task is to anticipate missing words or finish sentences given a particular context. For instance, with language modeling tasks, we may use prompts in the style of cloze to create coherent and suitable continuations for given text parts. In similar ways, for tasks like sentiment analysis or recognizing named entities in language understanding jobs - these kinds of prompts could give context-based hints that guide an AI model’s classification process (Bloomberg et al., 2020).

In total, cloze-style prompts show a well-arranged and efficient way for prompt engineering. They use the ideas of context-based guessing to improve language models' results in many tasks and areas. By creating cloze-style prompts that fit particular task goals and situation needs, people who study or use this field can make prompt engineering better to get more correct outputs from language models that also match with the context.

##### 3. Applications of Prompt Engineering:

Engineering finds application in various NLP tasks and domains, giving a flexible method to improve language models' performance. By customizing prompts for every task's requirements, this approach allows better use of pre-trained models and helps in adjusting them more effectively to the specific task. In this part, we will discuss some important uses of engineering:

##### 3.1 Question Answering:

Tasks for question answering (QA) involve giving precise and appropriate responses to questions in natural language. The performance of QA systems can be greatly enhanced by prompt engineering, which means creating prompts that direct the language model towards important details and provide brief yet informative reactions. For example, in a factoid-based QA task, a prompt may contain the question stem along with related context or hints to assist the model in recognizing suitable passages and extracting correct answers from them.

##### 3.2 Text Generation:

Tasks for generating text cover various applications like translating between languages, making summaries and creating new pieces of writing. Giving prompt instructions can improve models that generate text by offering clear suggestions or limitations to direct the process of generation. For instance, within abstractive summarization's setting, a prompt could state the needed length and main content to be present in summary. This would help make model produce summaries that are both more understandable and informative.

##### 3.3 Sentiment Analysis:

Tasks for sentiment analysis are about figuring out the feeling or emotion in a text, like positive, negative and neutral. Prompt engineering can enhance the precision of models for sentiment analysis by creating prompts that...
